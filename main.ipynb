{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "# %%capture, hides output of this cell. When cell fails to run remove capture to check error\n",
    "%pip install -r requirements.txt;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b2888",
   "metadata": {},
   "source": [
    "### Welcome\n",
    "Welcome to the main program. This notebook was developed using Python 3.12.2 in [VSCode](https://code.visualstudio.com/) and uses the packages listed in requirements.txt. These can be installed using the cell above. If that doesn't work, try creating a new virtual environment first.\n",
    "\n",
    "For an example you can can select to upload a demo preset file named [demo.pt](https://github.com/bytnater/bias-analyse/blob/main/presets/demo.pt). This will be in the folder `presets`. This file will analyze the attributes `persoon_geslacht_vrouw` and `persoon_leeftijd_bij_onderzoek` on every metric. It will use the dataset [`well_calibrated_bias_and_demo_file.csv`](https://github.com/bytnater/bias-analyse/blob/main/data/well_calibrated_bias_and_demo_file.csv) which should be in the folder data. This dataset simulates the prediction of a almost perfect model. The predictions are in favor of younger women. Please note that the predictions and actual outcome are fabricated for testing purposes.\n",
    "\n",
    "\n",
    "This notebook has the following cells\n",
    "- Parameter file and dataset selection\n",
    "- Ground truth, prediction, and feature selection\n",
    "- Metric selection\n",
    "- Calculations and Results\n",
    "- Saving parameters to file\n",
    "\n",
    "In the first cell, a dataset can be selected. This must be a `.csv` or a `.xlsx` file. You can also upload a parameter file in this cell. Such a file, which can be created at the end of this notebook, stores all the selections made during a previous session. When uploading a parameter file, the program will try to load the corresponding `.csv/.xlsx` file from the designated folder. If it is not found, you will be prompted to upload the dataset manually.\n",
    "\n",
    "In the second section, you can select the columns that contain predictions and ground truth. The program will give feedback indicating whether the column names have been found. Additionally, you can select protected attributes—each of these features will be evaluated separately by the selected metrics.\n",
    "\n",
    "In the next section, you can choose which metrics to use. Some metrics require either predictions and/or ground truth. If these requirements are not met, the corresponding metric will be disabled.\n",
    "\n",
    "As mentioned earlier, the final section allows you to save your selections to a parameter file. The program checks whether the filename is available and valid.\n",
    "\n",
    "It is recommended to run each cell and fill out the selections before running the next cell. There should be no need to change any code, all of the selection is done by [ipywidgets](https://ipywidgets.readthedocs.io/en/latest/how-to/index.html). The cells of the notebook can be closed by dubble clicking at the left edge of a cell. \n",
    "\n",
    "\n",
    "If you posses the model used you may also use SHAP to find the feature importance of the model. In the file, [shap fairness_explainer](https://github.com/bytnater/bias-analyse/blob/main/fairness_metrics/Fairness_explainer/shap_fairness_explainer.py), there is a example on how to use this modul.\n",
    "\n",
    "We also have created a small start to Hierarchical Bias-Aware Clustering ([HBAC](https://github.com/bytnater/bias-analyse/blob/main/HBAC.ipynb)). HBAC is designed to detect groups in data that may be treated unfairly—particularly in unsupervised settings (i.e., without known labels). It clusters the data based not on feature similarity alone, but on disparities in a chosen bias metric (e.g., fairness, outcome imbalance, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Box, VBox, Accordion, Label, Valid, Text, IntText, Combobox, Checkbox, RadioButtons, Button\n",
    "\n",
    "\n",
    "## part of the project\n",
    "from fairness_metrics.Predicted_outcomes.Error_rate_metrics import Error_rate_metrics\n",
    "from fairness_metrics.Predicted_outcomes.Predictive_value_metrics import Predictive_value_metrics\n",
    "from fairness_metrics.Predicted_outcomes.statistical_parity import statistical_parity\n",
    "from fairness_metrics.predicted_probs.balance_in_pos_neg import balance_in_pos_neg\n",
    "from fairness_metrics.predicted_probs.well_callibrated import well_calibration\n",
    "from fairness_metrics.similarity_based.similarity_based import LipschitzFairness\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e537fb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter file and dataset file selection\n",
    "\n",
    "# Radio buttons to choose between uploading a session file or starting a new one\n",
    "SESSION = RadioButtons(\n",
    "    options=[('Upload session file', 1), ('Create new session', 0)],\n",
    "    value=0,\n",
    "    style = {'description_width': 'initial'},\n",
    "    description='Load params file:',)\n",
    "\n",
    "# Define style and layout for widgets\n",
    "layout = Layout(width='auto')\n",
    "\n",
    "# UI box for uploading a .pt file (parameter file)\n",
    "PARAMS_UPLOAD_BOX = VBox([\n",
    "    Label(value='Upload your params file, this must be a .pt file'),\n",
    "    widgets.FileUpload(\n",
    "        accept='.pt',\n",
    "        description='Upload file',\n",
    "        tooltip='No file',\n",
    "        layout=layout,\n",
    "    )\n",
    "])\n",
    "\n",
    "# UI box for uploading a .csv file (dataset)\n",
    "DATA_UPLOAD_BOX = VBox([\n",
    "    Label(value='Please select the dataset you want to analyise, this must be a .csv file'),\n",
    "    widgets.FileUpload(\n",
    "        accept='.csv, .xlsx',\n",
    "        description='Upload file',\n",
    "        tooltip='No file',\n",
    "        layout=layout,\n",
    "    )\n",
    "])\n",
    "\n",
    "# Label for feedback messages during upload\n",
    "UPLOAD_FEEDBACK = Label(Value='')\n",
    "\n",
    "# Group all upload options and feedback into one vertical layout\n",
    "FILE_INFO_BOX = VBox([\n",
    "    SESSION,\n",
    "    PARAMS_UPLOAD_BOX,\n",
    "    DATA_UPLOAD_BOX,\n",
    "    UPLOAD_FEEDBACK,\n",
    "])\n",
    "\n",
    "# Add spacing below the radio buttons\n",
    "SESSION.layout.padding = '0px 0px 30px 0px'\n",
    "\n",
    "# Hide the parameter upload box by default\n",
    "PARAMS_UPLOAD_BOX.layout.display = 'None'\n",
    "\n",
    "output_file_selection = widgets.Output()\n",
    "display(FILE_INFO_BOX, output_file_selection)\n",
    "\n",
    "# Function to toggle between session options\n",
    "def toggle(change):\n",
    "    if SESSION.value:\n",
    "        # If user chooses to upload a session file\n",
    "        UPLOAD_FEEDBACK.value = ''\n",
    "        PARAMS_UPLOAD_BOX.layout.display = 'block'\n",
    "        DATA_UPLOAD_BOX.layout.display = 'none'\n",
    "    else: \n",
    "        # If user chooses to create a new session\n",
    "        UPLOAD_FEEDBACK.value = ''\n",
    "        DATA_UPLOAD_BOX.layout.display = 'block'\n",
    "        PARAMS_UPLOAD_BOX.layout.display = 'none'\n",
    "        PARAMS_UPLOAD_BOX.children[1].value = ()\n",
    "        PARAMS_UPLOAD_BOX.children[1].description='Upload file'\n",
    "        PARAMS_UPLOAD_BOX.children[1].tooltip='No file'\n",
    "\n",
    "        # Save an empty parameter file\n",
    "        params = {}\n",
    "        torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n",
    "\n",
    "# Function to handle uploaded .pt parameter files\n",
    "def upload_param_file(change):\n",
    "    with output_file_selection:\n",
    "        if change['new']:\n",
    "            uploaded_file = change['new'][0]\n",
    "            # Update file upload UI text\n",
    "            PARAMS_UPLOAD_BOX.children[1].description = f'Uploaded \"{uploaded_file.name}\"'\n",
    "            PARAMS_UPLOAD_BOX.children[1].tooltip = f'{uploaded_file.name}'\n",
    "\n",
    "            # Load parameter file from uploaded content\n",
    "            params = torch.load(BytesIO(uploaded_file.content), weights_only=True)\n",
    "            used_dataset = params.get('used_dataset', '')\n",
    "\n",
    "            if not used_dataset:\n",
    "                # If no dataset info is in the parameter file, ask user to upload one\n",
    "                DATA_UPLOAD_BOX.layout.display = 'block'\n",
    "                UPLOAD_FEEDBACK.value = f'No dataset file found connected to this preset file, please upload a file manually'\n",
    "            else:\n",
    "                # Try to load the dataset from disk\n",
    "                if os.path.exists(utils.SAVED_DATASET_PATH + used_dataset):\n",
    "                    global dataset\n",
    "                    dataset = utils.Dataset(utils.SAVED_DATASET_PATH + used_dataset)\n",
    "\n",
    "                    # Reset the dataset upload UI\n",
    "                    DATA_UPLOAD_BOX.children[1].value = ()\n",
    "                    DATA_UPLOAD_BOX.children[1].description='Upload file'\n",
    "                    DATA_UPLOAD_BOX.children[1].tooltip='No file'\n",
    "                    DATA_UPLOAD_BOX.layout.display = 'none'\n",
    "\n",
    "                    UPLOAD_FEEDBACK.value = f'Dataset file found at \"{utils.SAVED_DATASET_PATH + used_dataset}\"'\n",
    "                else:\n",
    "                    # If dataset file is missing on disk, ask for manual upload\n",
    "                    DATA_UPLOAD_BOX.layout.display = 'block'\n",
    "                    UPLOAD_FEEDBACK.value = f'file \"{used_dataset}\" not found at \"{utils.SAVED_DATASET_PATH + used_dataset}\", please upload a file manually'\n",
    "            \n",
    "            # Save the parameter file for later use\n",
    "            torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n",
    "\n",
    "# Function to handle uploaded .csv dataset files\n",
    "def upload_csv(change):\n",
    "    with output_file_selection:\n",
    "        if change['new']:\n",
    "            uploaded_file = change['new'][0]\n",
    "\n",
    "            # Update file upload UI text\n",
    "            DATA_UPLOAD_BOX.children[1].description = f'Uploaded \"{uploaded_file.name}\"'\n",
    "            DATA_UPLOAD_BOX.children[1].tooltip = f'{uploaded_file.name}'\n",
    "\n",
    "            global dataset\n",
    "            dataset = utils.Dataset(upload_widget=change['new'])\n",
    "\n",
    "            # Update and save the dataset name in the parameter file\n",
    "            params = torch.load(utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET, weights_only=True)\n",
    "            params['used_dataset'] = uploaded_file.name\n",
    "            torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n",
    "\n",
    "# Connect widgets to their corresponding functions      \n",
    "SESSION.observe(toggle, names='value')\n",
    "PARAMS_UPLOAD_BOX.children[1].observe(upload_param_file, names='value')\n",
    "DATA_UPLOAD_BOX.children[1].observe(upload_csv, names='value')\n",
    "\n",
    "# Reset dataset\n",
    "dataset=None\n",
    "\n",
    "# Reset session parameter file\n",
    "params = {}\n",
    "torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b632587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth, prediction, and protected atrributes selection\n",
    "\n",
    "# Make sure a dataset is selected\n",
    "assert dataset, 'Make sure to select a dataset to analise'\n",
    "\n",
    "# Load saved parameters\n",
    "params = torch.load(utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET, weights_only=True)\n",
    "\n",
    "# Define style and layout for widgets\n",
    "style = {'background':'white'}\n",
    "item_layout = Layout(width='auto')\n",
    "\n",
    "# Create a checkbox for each feature to select as a protected attribute\n",
    "protected_items = [Checkbox(layout=item_layout, description=dataset.i2c[i], indent=False, style=style) for i in range(len(dataset.i2c))]\n",
    "\n",
    "# Layout settings for the box that contains all checkboxes\n",
    "box_layout = Layout(\n",
    "    overflow='hidden scroll',\n",
    "    border='empty',\n",
    "    width='auto',\n",
    "    height='300px',\n",
    "    flex_flow='column',\n",
    "    display='flex',\n",
    "    padding='0'\n",
    ")\n",
    "\n",
    "# Box that displays all protected attribute checkboxes\n",
    "PROTECTED_SELECTION = Box(children=protected_items, layout=box_layout)\n",
    "\n",
    "# Combobox for selecting the ground truth feature\n",
    "GROUND_SELECTION = Combobox(\n",
    "    value=None,\n",
    "    placeholder='Choose a feature',\n",
    "    options=dataset.i2c+[''],\n",
    "    description='Ground truth:',\n",
    "    ensure_option=True,\n",
    "    style={'description_width':'150px'},\n",
    "    layout=Layout(width='auto'),\n",
    ")\n",
    "\n",
    "# Combobox for selecting the predicted feature\n",
    "PREDICTED_SELECTION = Combobox(\n",
    "    value=None,\n",
    "    placeholder='Choose a feature',\n",
    "    options=dataset.i2c+[''],\n",
    "    description='Predicted by model:',\n",
    "    ensure_option=True,\n",
    "    style={'description_width':'150px'},\n",
    "    layout=Layout(width='auto'),\n",
    ")\n",
    "\n",
    "# Label to show feedback about the selected features\n",
    "FEEDBACK_INFO_SLECETION = Label(value='')\n",
    "FEEDBACK_INFO_SLECETION.layout.padding = '0px 0px 90px 0px'\n",
    "\n",
    "# Combine all widgets into a single vertical layout\n",
    "SELECTION_BOX = VBox([\n",
    "    Label('Please select the ground truth and the predicted probability (if available):'), \n",
    "    GROUND_SELECTION,\n",
    "    PREDICTED_SELECTION,\n",
    "    FEEDBACK_INFO_SLECETION,\n",
    "    Label('Please, select the protected attributes you want to analyse:'), \n",
    "    PROTECTED_SELECTION,\n",
    "    ])\n",
    "\n",
    "# Restore previously saved values for protected attributes and selected columns\n",
    "preset_protected_values = params.get('protected_values', torch.zeros(len(dataset.i2c), dtype=bool))\n",
    "for item, value in zip(protected_items, preset_protected_values):\n",
    "    item.value = bool(value)\n",
    "GROUND_SELECTION.value=params.get('ground_truth_column', '')\n",
    "PREDICTED_SELECTION.value=params.get('prediction_column', '')\n",
    "\n",
    "# Display output\n",
    "output_selection = widgets.Output()\n",
    "display(SELECTION_BOX, output_selection)\n",
    "\n",
    "# Function to update feedback based on current selection\n",
    "def update_selection_feedback(id):\n",
    "    with output_selection:\n",
    "        if GROUND_SELECTION.value == PREDICTED_SELECTION.value != '':\n",
    "            FEEDBACK_INFO_SLECETION.value=f'Both are now selected as \"{GROUND_SELECTION.value}\", Please make sure that the ground truth is not the same as the predictions'\n",
    "        else:\n",
    "            FEEDBACK_INFO_SLECETION.value = (\n",
    "                'You have selected ' + \n",
    "                (f'ground truth as \"{GROUND_SELECTION.value}\"' if GROUND_SELECTION.value else 'no ground truth') + \n",
    "                ' and ' + \n",
    "                (f'predictions as \"{PREDICTED_SELECTION.value}\"' if PREDICTED_SELECTION.value else 'no predictions')\n",
    "            )\n",
    "\n",
    "# Link feedback update function to changes in the boxes\n",
    "GROUND_SELECTION.on_trait_change(update_selection_feedback)\n",
    "PREDICTED_SELECTION.on_trait_change(update_selection_feedback)\n",
    "SELECTION_BOX.on_widget_constructed(update_selection_feedback)\n",
    "\n",
    "# Button to save the current selection\n",
    "SAVE_CHANGES = Button(\n",
    "    description='Save changes',\n",
    "    button_style='success',\n",
    "    tooltip='Save your current selection',\n",
    ")\n",
    "\n",
    "# Display output for saving\n",
    "output_save_changes = widgets.Output()\n",
    "display(SAVE_CHANGES, output_save_changes)\n",
    "\n",
    "# Function to save the current selection back into the preset file\n",
    "def save_changes(id):\n",
    "    with output_save_changes:\n",
    "        params['protected_values'] = torch.tensor([protected_item.value for protected_item in protected_items])\n",
    "        params['ground_truth_column'] = GROUND_SELECTION.value\n",
    "        params['prediction_column'] = PREDICTED_SELECTION.value\n",
    "        torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n",
    "\n",
    "# Link the save function to the button click\n",
    "SAVE_CHANGES.on_click(save_changes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cda1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric selection\n",
    "\n",
    "# Load saved parameters\n",
    "params = torch.load(utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET, weights_only=True)\n",
    "\n",
    "# Makes sure that at least one attribute is being analyzed\n",
    "assert sum(params.get('protected_values', torch.zeros(len(dataset.i2c), dtype=bool))) != 0, 'You have to select at least one protected attribute'\n",
    "\n",
    "# Style and layout\n",
    "item_layout = Layout(width='auto')\n",
    "slider_style = {'description_width': '200px'}\n",
    "\n",
    "# Descriptions\n",
    "fairness_groups_names = [\n",
    "    'Predicted and actual outcomes',\n",
    "    'Predicted probibilities and actual outcomes',\n",
    "    'Similarity based',\n",
    "]\n",
    "metric_names = [\n",
    "    ['Error rate', 'Predictive value', 'Statistical parity'],\n",
    "    ['Test-fairness', 'Balance in classes'],\n",
    "    ['Lipschitz']\n",
    "]\n",
    "\n",
    "# Valid widgets\n",
    "VALID_GROUND = Valid(value=True, description='Ground truth')\n",
    "VALID_PRED = Valid(value=True, description='Prediction')\n",
    "\n",
    "# Load slider cache: tag -> slider\n",
    "slider_cache = {\n",
    "    name:widgets.FloatSlider(\n",
    "        value=value,\n",
    "        min=0.1, max=5, step=0.1,\n",
    "        description=name,\n",
    "        tooltip=name,\n",
    "        layout=item_layout,\n",
    "        indent=False,\n",
    "        style=slider_style)\n",
    "    for name, value in params.get('condition', dict()).items()\n",
    "}\n",
    "\n",
    "# Tag selector\n",
    "value = [name for (name, slider) in slider_cache.items()]\n",
    "tags = widgets.TagsInput(\n",
    "    value=value,\n",
    "    allowed_tags=dataset.i2c,\n",
    "    allow_duplicates=False\n",
    ")\n",
    "\n",
    "# Container for the sliders\n",
    "box_layout = Layout(\n",
    "    overflow='hidden scroll',\n",
    "    border='empty',\n",
    "    width='auto',\n",
    "    max_height='300px',\n",
    "    flex_flow='column',\n",
    "    display='flex',\n",
    "    padding='0'\n",
    ")\n",
    "\n",
    "children = [slider for (name, slider) in slider_cache.items()]\n",
    "TAGS_SLIDERS = Box(children=children, layout=box_layout)\n",
    "\n",
    "# TAG_Wrapper for display\n",
    "TAG_WRAPPER = VBox([\n",
    "    tags,\n",
    "    TAGS_SLIDERS,\n",
    "])\n",
    "\n",
    "# Dynamic update function\n",
    "def update_tags_sliders(change):\n",
    "    if change['name'] == 'value':\n",
    "        current_tags = change['new']\n",
    "        new_sliders = []\n",
    "\n",
    "        for name in current_tags:\n",
    "            if name in slider_cache:\n",
    "                slider = slider_cache[name]\n",
    "            else:\n",
    "                slider = widgets.FloatSlider(\n",
    "                    value=1.0,\n",
    "                    min=0.1, max=5, step=0.1,\n",
    "                    description=name,\n",
    "                    tooltip=name,\n",
    "                    layout=item_layout,\n",
    "                    indent=False,\n",
    "                    style=slider_style,\n",
    "                )\n",
    "                slider_cache[name] = slider\n",
    "\n",
    "            new_sliders.append(slider)\n",
    "\n",
    "        TAGS_SLIDERS.children = new_sliders\n",
    "\n",
    "# Observe changes to the tags input\n",
    "tags.observe(update_tags_sliders, names='value')\n",
    "\n",
    "# Metric widgets\n",
    "TEST_FAIRNESS = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='This metric check if individuals assigned the same score s have equal likelihoods of the positive outcome, independent of the protected attribute'),\n",
    "    VBox([VALID_GROUND, VALID_PRED]),\n",
    "])\n",
    "BALANCE_IN_CLASS = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='Balance for the positive and negative classes are fairness criteria that focus on ensuring equitable scoring among individuals who share the same actual outcome.'),\n",
    "    Checkbox(layout=item_layout, description='Show the positive balance', indent=False),\n",
    "    Checkbox(layout=item_layout, description='Show the negative balance', indent=False),\n",
    "    VBox([VALID_GROUND, VALID_PRED]),\n",
    "])\n",
    "ERROR_RATE_METRICS = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='Please, make a selections'),\n",
    "    VBox([VALID_GROUND, VALID_PRED]),\n",
    "])\n",
    "PREDICTIVE_VALUE_METRICS = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='Please, make a selections'),\n",
    "    VBox([VALID_GROUND, VALID_PRED]),\n",
    "])\n",
    "STATISTICAL_PARITY = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='Please, make a selections'),\n",
    "    TAG_WRAPPER,\n",
    "    VBox([VALID_PRED]),\n",
    "])\n",
    "SIMILARITY_BASED = VBox([\n",
    "    Checkbox(layout=item_layout, value=True, description='Use metric?', indent=False),\n",
    "    Label(value='Please, make a selections'),\n",
    "    IntText(value=1000, description='Sample size:'),\n",
    "    RadioButtons(options=['Manhattan Distance', 'Euclidean Distance', 'cosine'],\n",
    "                 value='Manhattan Distance',\n",
    "                 style={'description_width': 'initial'},\n",
    "                 description='Choose distance metric:'),\n",
    "    VBox([VALID_PRED]),\n",
    "])\n",
    "\n",
    "# Define metrics\n",
    "metrics_groups = [\n",
    "    [ERROR_RATE_METRICS, PREDICTIVE_VALUE_METRICS, STATISTICAL_PARITY],\n",
    "    [TEST_FAIRNESS, BALANCE_IN_CLASS],\n",
    "    [SIMILARITY_BASED]\n",
    "]\n",
    "\n",
    "# Utility to simulate an accordion group with multi-open toggle buttons\n",
    "def make_toggle_section(title, content):\n",
    "    toggle = widgets.ToggleButton(value=False, description=title, layout=Layout(width='200px'))\n",
    "    box = VBox([content])\n",
    "    box.layout.display = 'block'\n",
    "\n",
    "    def toggle_visibility(change):\n",
    "        box.layout.display = 'none' if change['new'] else 'block'\n",
    "\n",
    "    toggle.observe(toggle_visibility, names='value')\n",
    "    return VBox([toggle, box])\n",
    "\n",
    "# Apply toggle accordion logic to each metrics group\n",
    "custom_groups = []\n",
    "for group_metrics, titles in zip(metrics_groups, metric_names):\n",
    "    group_box = VBox([\n",
    "        make_toggle_section(title, content)\n",
    "        for title, content in zip(titles, group_metrics)\n",
    "    ])\n",
    "    custom_groups.append(group_box)\n",
    "\n",
    "# Create outer accordion using standard ipywidgets (since you only want one top-level group open)\n",
    "outer_accordion = Accordion(children=custom_groups)\n",
    "for i, title in enumerate(fairness_groups_names[:len(custom_groups)]):\n",
    "    outer_accordion.set_title(i, title)\n",
    "\n",
    "# Disables metric that are not usable with the current selection\n",
    "def validate_valid_valids(metric_widgets):\n",
    "    valid_valids = all(v.value for v in metric_widgets.children[-1].children)\n",
    "    metric_widgets.children[0].value = valid_valids\n",
    "    metric_widgets.children[0].disabled = not(valid_valids)\n",
    "\n",
    "# Restore state\n",
    "VALID_GROUND.value = bool(params.get('ground_truth_column', ''))\n",
    "VALID_PRED.value = bool(params.get('prediction_column', ''))\n",
    "\n",
    "[[validate_valid_valids(metric) for metric in group] for group in metrics_groups] \n",
    "\n",
    "default = [[metric.children[0].value for metric in group] for group in metrics_groups]\n",
    "metric_selection = params.get('metric_selection', default)\n",
    "for group, group_selection in zip(metrics_groups, metric_selection):\n",
    "    for metric, selection in zip(group, group_selection):\n",
    "        if not metric.children[0].disabled:\n",
    "            metric.children[0].value = selection\n",
    "\n",
    "BALANCE_IN_CLASS.children[2].value = params.get('balance_pos', True)\n",
    "BALANCE_IN_CLASS.children[3].value = params.get('balance_neg', True)\n",
    "SIMILARITY_BASED.children[2].value = params.get('sample_limit', 1000)\n",
    "SIMILARITY_BASED.children[3].value = params.get('distance_metric', 'Manhattan Distance')\n",
    "\n",
    "# Display the result\n",
    "display(outer_accordion)\n",
    "\n",
    "# Button to save the current selection\n",
    "SAVE_CHANGES = Button(\n",
    "    description='Save changes',\n",
    "    button_style='success',\n",
    "    tooltip='Save your current selection',\n",
    ")\n",
    "\n",
    "# Display output for saving\n",
    "output_save_changes = widgets.Output()\n",
    "display(SAVE_CHANGES, output_save_changes)\n",
    "\n",
    "# Function to save the current selection back into the preset file\n",
    "def save_changes(id):\n",
    "    with output_save_changes:\n",
    "        params['metric_selection'] = [[metric.children[0].value for metric in group] for group in metrics_groups]\n",
    "        params['balance_pos'] = BALANCE_IN_CLASS.children[2].value\n",
    "        params['balance_neg'] = BALANCE_IN_CLASS.children[3].value\n",
    "        params['sample_limit'] = SIMILARITY_BASED.children[2].value\n",
    "        params['distance_metric'] = SIMILARITY_BASED.children[3].value\n",
    "        params['condition'] = {slider.description:slider.value for slider in TAGS_SLIDERS.children}\n",
    "        torch.save(params, utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET)\n",
    "\n",
    "# Link the save function to the button click\n",
    "SAVE_CHANGES.on_click(save_changes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0fbcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and calculating each metric \n",
    "\n",
    "# Load saved parameters\n",
    "params = torch.load(utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET, weights_only=True)\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "# Defining metrics\n",
    "metrics = [[Error_rate_metrics, Predictive_value_metrics, statistical_parity],\n",
    "           [well_calibration, balance_in_pos_neg],\n",
    "           [LipschitzFairness]]\n",
    "\n",
    "# Select and calculate metrics\n",
    "default = [[metric.children[0].value for metric in group] for group in metrics_groups]\n",
    "metric_selection = params.get('metric_selection', default)\n",
    "used_metrics = []\n",
    "for metric, metric_name, selection in zip(flatten(metrics), flatten(metric_names), flatten(metric_selection)):\n",
    "    if selection:\n",
    "        used_metrics.append((metric(dataset, params), metric_name))\n",
    "        print(f'{metric_name} calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744a2a4",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Running the codecell below will Produce a graphs per feature for each metric in your selection:\n",
    "\n",
    "The Error rate metric will produce a graph showing the False Positive Rate (FPR) and False Negative Rate (FNR) for each group in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Predictive Equality, which requires equal FPRs accross all groups.\n",
    "- Equal Opportunity, which requires equal FNRs accross all groups.\n",
    "- Equalised Odds, which requires equal FPRs and FNRs accross all groups.\n",
    "\n",
    "The Predictive value metric will produce a graph showing the Positive Predictive Value (PPV) and Negative Predictive Value (NPV) for each group in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Predictive Parity, which requires equal PPVs accross all groups.\n",
    "- Conditional Use Accuracy Equality, which requires equal PPVs and NPVs accross all groups.\n",
    "\n",
    "The (conditional) Statistical Parity metric will produce a graph showing the Positive Prediction rate for each group in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Statistical Parity, which requires equal Positive Prediction rates accross all groups.\n",
    "- Conditional Statistical Parity, which requires equal Positive Prediction rates of individuals that conform to the condition accross all groups.\n",
    "\n",
    "The Test-fairness metric will produce a graph showing the actual and predicted probability of an positive outcome for each group in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Test-fairness, which requires that for any given predicted risk score, the probability of the positive outcome is equal across all groups.\n",
    "- Well-calibration, which requires that for any given predicted risk score, the probability of the positive outcome is equal across all groups and the predicted risk score accurately reflects the probability of the positive outcome.\n",
    "\n",
    "The Balance in classes metric will produce a graph showing the predicted risk score of the positve and negative class for each group in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Balance For The Positive Class, which requires average equal predicted risk scores for the positve class accross all groups.\n",
    "- Balance For The Negative Class, which requires average equal predicted risk scores for the negative class accross all groups.\n",
    "\n",
    "The Lipschitz metric will produce a graph showing the severity of Lipschitz violations for all samples in the attribute. This allow you to check fairness accoring to the following metrics:\n",
    "- Fairness Through Awareness, which requires that the difference in outputs is no greater than a constant multiple of the difference in inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b98ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show results of each metric\n",
    "for metric, metric_name in used_metrics:\n",
    "    print(f'{'#' * (len(metric_name) + 11)}\\n# Metric {metric_name} #\\n{'#' * (len(metric_name) + 11)}')\n",
    "    for image in metric.show():\n",
    "        if isinstance(image, go.Figure):\n",
    "            image.show()\n",
    "        else:\n",
    "            print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949e8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving parameters to file\n",
    "\n",
    "# Info Widgets \n",
    "OUTPUT_FILE_NAME = Text(\n",
    "    placeholder='Type output file name',\n",
    "    description='File name:',\n",
    "    tooltip='do not forget the file extension'\n",
    ")\n",
    "FEEDBACK_INFO = Label(\n",
    "    value='',\n",
    ")\n",
    "SUBMIT_BUTTON = Button(\n",
    "    description='Save preset',\n",
    "    disabled=True,\n",
    "    button_style='danger',\n",
    "    tooltip='No file name',\n",
    ")\n",
    "info_box = VBox([\n",
    "    Label(value=f'Select a name for your preset file here, This will be saved in the folder \"{utils.SAVED_PRESET_PATH}\"'),\n",
    "    OUTPUT_FILE_NAME,\n",
    "    SUBMIT_BUTTON,\n",
    "    FEEDBACK_INFO,\n",
    "])\n",
    "\n",
    "# Display widgets\n",
    "output_savefile = widgets.Output()\n",
    "display(info_box, output_savefile)\n",
    "\n",
    "def update_file_name_info(id):\n",
    "    with output_savefile:\n",
    "        if type(id) == Button:\n",
    "            params = torch.load(utils.SAVED_PRESET_PATH + utils.RESEVERD_PRESET, weights_only=True)\n",
    "\n",
    "            torch.save(params, utils.SAVED_PRESET_PATH + OUTPUT_FILE_NAME.value)\n",
    "            FEEDBACK_INFO.value=f'Preset saved to \"{OUTPUT_FILE_NAME.value}\"'\n",
    "        \n",
    "        if type(id) == Text:\n",
    "            FEEDBACK_INFO.value=''\n",
    "        \n",
    "        file_exits = os.path.exists(utils.SAVED_PRESET_PATH + OUTPUT_FILE_NAME.value)\n",
    "        if OUTPUT_FILE_NAME.value == '':\n",
    "            SUBMIT_BUTTON.button_style='danger'\n",
    "            SUBMIT_BUTTON.tooltip='No file name'\n",
    "            SUBMIT_BUTTON.disabled=True\n",
    "        elif OUTPUT_FILE_NAME.value == 'session_save.pt':\n",
    "            SUBMIT_BUTTON.button_style='danger'\n",
    "            SUBMIT_BUTTON.tooltip='This file name is reserved'\n",
    "            SUBMIT_BUTTON.disabled=True\n",
    "        elif not OUTPUT_FILE_NAME.value.endswith('.pt'):\n",
    "            SUBMIT_BUTTON.button_style='warning'\n",
    "            SUBMIT_BUTTON.tooltip='Please make it a .pt file'\n",
    "            SUBMIT_BUTTON.disabled=False\n",
    "        elif file_exits:\n",
    "            SUBMIT_BUTTON.button_style='info'\n",
    "            SUBMIT_BUTTON.tooltip='A file already exists with this name, overwrite is possible'\n",
    "            SUBMIT_BUTTON.disabled=False\n",
    "        else:\n",
    "            SUBMIT_BUTTON.button_style='success'\n",
    "            SUBMIT_BUTTON.tooltip='Save file'\n",
    "            SUBMIT_BUTTON.disabled=False\n",
    "\n",
    "# Link feedback update function to changes in the boxes\n",
    "OUTPUT_FILE_NAME.on_trait_change(update_file_name_info)\n",
    "SUBMIT_BUTTON.on_click(update_file_name_info)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
